重启 Ubuntu 后敲入 `podman` 及 `kubectl` 命令时报错，内容如下

```sh
~ > podman ps
ERRO[0000] invalid internal status, try resetting the pause process with "podman system migrate": could not find any running process: no such process
~ > kubectl get node
The connection to the server 127.0.0.1:35703 was refused - did you specify the right host or port?
```

此问题是由 Ubuntu 的「用户驻留」功能处于关闭状态导致的。

### 开启 Linger

可以通过登录控制命令 Login Control 开启或关闭用户驻留功能

```sh
# 以下命令中的 $(whoami) 其实就是当前用户的名称

# 查询当前用户的驻留功能是否已开启
~ > loginctl show-user $(whoami) | grep -i linger
Linger=no

# 开启用户驻留功能
~ > loginctl enable-linger $(whoami)
~ > loginctl show-user $(whoami) | grep -i linger
Linger=yes

# 与开启命令相对的关闭命令写法如下
# loginctl disable-linger $(whoami)
```

### 修复或重置 `podman`

```sh
# 修复命令
podman system migrate
# 重新同步 Podman 的内部状态，清理掉过期的锁文件和进程记录，并重新映射用户命名空间

# 重置命令
# podman system reset
# 将 Podman 恢复到最初安装的状态（也会删除已下载的镜像）。
```

对于 `invalid internal status` 报错，执行 `migrate` 命令就可以解决。如果不奏效，改用 `reset` 命令。

### 重建 K8s 集群

```sh
kind delete cluster
kind create cluster --config kind-config.yaml
```

### 验证

重启 Ubuntu 后重启执行两条命令，发现 podman 没问题，但是 K8s 集群依然报错

```sh
~ > kubectl get node
The connection to the server 127.0.0.1:39963 was refused - did you specify the right host or port?
~ > podman ps
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES
~ >
~ >
```

这是因为 kind 通过容器模拟的 K8s 集群在 Ubuntu 开机后并没有自启动引起的。通过手动启动集群的控制平面就可以验证这一点

```sh
# 启动控制平面
~ > podman start kind-control-plane
kind-control-plane
~ >

# 此时可以正确获取节点信息
~ > kubectl get node
NAME                 STATUS     ROLES           AGE   VERSION
kind-control-plane   Ready      control-plane   42m   v1.35.0
kind-worker          NotReady   <none>          41m   v1.35.0
kind-worker2         NotReady   <none>          41m   v1.35.0
```

注意：工作节点全部处于 NotReady 状态。这说明对于多节点集群，仅仅启动控制平面是不够的，所有的节点容器都应该启动。

### 设置集群开机自启动

实现所有节点开机自启动需要分情况讨论。

#### 对于 docker

可以通过以下命令实现节点自启动

```sh
# 注意：这条命令只需要执行一次
docker update --restart always kind-control-plane kind-worker kind-worker2
```

我的虚拟机并没安装 docker，无法验证这条命令是否有效。

#### 对于 `podman`

可以使用传统的 `@reboot` 任务：

1. 输入 `crontab -e`（如果是第一次，选 `nano` 即可）。
2. 在文件末尾添加这一行：

   ```sh
   @reboot podman start kind-control-plane kind-worker kind-worker2
   ```

3. 保存退出。

经验证，此法有效，重启虚拟机后 `kubectl` 命令可以正常执行。
